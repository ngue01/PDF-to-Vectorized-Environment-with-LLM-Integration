{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain transformers sentence-transformers faiss-cpu huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Fonction gestion de texte en morceaux plus petit (chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avec le taux de tokens en entre, on pense a diviser le tokens par des chunk de 1000 a l'entree du model\n",
    "def split_text(text, chunk_size=1000):\n",
    "    # Diviser le texte en morceaux de taille chunk_size\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Charger les documents à indexer\n",
    "# Ici, nous utilisons un exemple de documents textuels stockés localement\n",
    "loader = TextLoader(\"Laos, 10 September 2024.txt\")  # Charger des documents\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnel : Combiner tous les documents en un seul texte (si nécessaire)\n",
    "combined_text = \" \".join([doc.page_content for doc in documents])\n",
    "\n",
    "# Diviser le texte combiné en chunks de 1000 caractères\n",
    "chunks = split_text(combined_text, chunk_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etant donner que langchain s'apprete a recevoir des objects de types document, nous devons convertir chaque chunk (chaîne de caractères) en un objet de type Document avant de les passer à FAISS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Convertir chaque chunk en objet Document pour Langchain\n",
    "documents_chunks = [Document(page_content=chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder\\envs\\mon_gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Embeddings - Transformer les documents en vecteurs avec Hugging Face\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creer maintenent une indexation vectorielle avec que le FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un index vectoriel avec FAISS pour les chunks\n",
    "vector_store = FAISS.from_documents(documents_chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Utiliser un modèle Hugging Face pour la génération de texte (GPT ou autre)\n",
    "llm = HuggingFaceHub(repo_id=\"gpt2\", model_kwargs={\"temperature\": 0.7, \"max_length\": 512, \"max_new_tokens\": 50}, huggingfacehub_api_token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Configurer le système de récupération avec Langchain\n",
    "qa_system = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", #\n",
    "    retriever=vector_store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de démonstration : Posez une question et traitez chaque chunk séparément\n",
    "def ask_question(query, chunk_size=1000):\n",
    "    # Diviser la question en morceaux plus petits si nécessaire\n",
    "    query_chunks = split_text(query, chunk_size=chunk_size)\n",
    "\n",
    "    # Traiter chaque chunk de la question séparément\n",
    "    for chunk in query_chunks:\n",
    "        response = qa_system.run(chunk)\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      " look forward to discussing my application and contributing to the success of your projects.\n",
      "\n",
      "Yours sincerely,\n",
      "\n",
      "NGUEMDJOM Kevin\n",
      "\n",
      "Laos, 10 September 2024\n",
      "NGUEMDJOM Tchangang\n",
      "Kevin Darren\n",
      "Hanoi-Vietnam\n",
      "tchangang25@gmail.com\n",
      "\n",
      "Subject: Application for the internship on clock synchronization in WBAN sensor networks\n",
      "\n",
      "Dear Sir/Madam,\n",
      "\n",
      "I am currently a second-year Master's student in Intelligent Systems and Multimedia, pursuing a dual degree between the National University of Vietnam and the University of La Rochelle in France. I would like to submit my application for the internship you are offering on the study of clock synchronization in Wireless Body Area Networks (WBAN) sensor networks.\n",
      "\n",
      "My academic background and experiences have allowed me to develop expertise in IoT systems, sensor management, and communication gateway development. During my studies, I have had the opportunity to participate in several projects, including at the Research Institute for Development (IRD) in Laos, where I am currently doing an internship. This internship focuses on the integration and configuration of sensors to measure temperature\n",
      "\n",
      ", air and soil humidity, as well as CO2 emissions. I have also worked on the creation of Raspberry Pi gateways with LoRa technology and the adaptation of firmware in the Arduino environment using C/C++.\n",
      "\n",
      "I am particularly interested in your project, as it perfectly aligns with my current work on real-time data transmission and the management of possible disturbances in complex environments. Indeed, in Laos, I am already working on similar issues related to long-distance communication between sensors and implementing low-cost solutions for environmental data collection. This internship would represent a unique opportunity for me to apply my skills in a new setting and further develop my expertise on clock synchronization, which is particularly important in wireless sensor networks. It would also allow me to complete my master's program and pursue a Ph.D. opportunity.\n",
      "\n",
      "I am confident that my skills in C/C++, sensor networks, and LoRa and WBAN technologies will be an asset to your team. I\n",
      "\n",
      "Question: What is this document about?\n",
      "Helpful Answer:\n",
      "\n",
      "If you are interested in an interview, please write to:\n",
      "\n",
      "Yuan Lao\n",
      "\n",
      "Director\n",
      "\n",
      "Yuan Lab\n",
      "\n",
      "Tel: 878-637-4111\n",
      "\n",
      "Here, you can also write, at\n"
     ]
    }
   ],
   "source": [
    "# Exemple de question : générer une réponse pour chaque chunk\n",
    "ask_question(\"What is this document about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder\\envs\\mon_gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Laos, 10 September 2024\n",
      "NGUEMDJOM Tchangang\n",
      "Kevin Darren\n",
      "Hanoi-Vietnam\n",
      "tchangang25@gmail.com\n",
      "\n",
      "Subject: Application for the internship on clock synchronization in WBAN sensor networks\n",
      "\n",
      "Dear Sir/Madam,\n",
      "\n",
      "I am currently a second-year Master's student in Intelligent Systems and Multimedia, pursuing a dual degree between the National University of Vietnam and the University of La Rochelle in France. I would like to submit my application for the internship you are offering on the study of clock synchronization in Wireless Body Area Networks (WBAN) sensor networks.\n",
      "\n",
      "My academic background and experiences have allowed me to develop expertise in IoT systems, sensor management, and communication gateway development. During my studies, I have had the opportunity to participate in several projects, including at the Research Institute for Development (IRD) in Laos, where I am currently doing an internship. This internship focuses on the integration and configuration of sensors to measure temperature, air and soil humidity, as well as CO2 emissions. I have also worked on the creation of Raspberry Pi gateways with LoRa technology and the adaptation of firmware in the Arduino environment using C/C++.\n",
      "\n",
      "I am particularly interested in your project, as it perfectly aligns with my current work on real-time data transmission and the management of possible disturbances in complex environments. Indeed, in Laos, I am already working on similar issues related to long-distance communication between sensors and implementing low-cost solutions for environmental data collection. This internship would represent a unique opportunity for me to apply my skills in a new setting and further develop my expertise on clock synchronization, which is particularly important in wireless sensor networks. It would also allow me to complete my master's program and pursue a Ph.D. opportunity.\n",
      "\n",
      "I am confident that my skills in C/C++, sensor networks, and LoRa and WBAN technologies will be an asset to your team. I look forward to discussing my application and contributing to the success of your projects.\n",
      "\n",
      "Yours sincerely,\n",
      "\n",
      "NGUEMDJOM Kevin\n",
      "\n",
      "Question: What is this document about?\n",
      "Helpful Answer:\n",
      "\n",
      "This document is an excerpt from \"Introduction to the WAN Signal Processing System and its Applications\".\n",
      "\n",
      "\n",
      "In this document, I will show how clock synchronization can be implemented with LoRa and other hardware sensors, in a way that allows for an intelligent, scalable and flexible approach to monitoring, monitoring, and recording of temperature, humidity, and air quality, as well as monitoring and recording of other things like radio emissions and temperature. It also shows how to implement a smart data management system which\n"
     ]
    }
   ],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. Charger les documents à indexer\n",
    "# Ici, nous utilisons un exemple de documents textuels stockés localement\n",
    "loader = TextLoader(\"Laos, 10 September 2024.txt\")  # Charger des documents\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Embeddings - Transformer les documents en vecteurs avec Hugging Face\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 3. Créer un index vectoriel avec FAISS\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# 4. Utiliser un modèle Hugging Face pour la génération de texte (GPT ou autre)\n",
    "llm = HuggingFaceHub(repo_id=\"gpt2\", model_kwargs={\"temperature\": 0.7, \"max_length\": 512}, huggingfacehub_api_token=\"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\")\n",
    "\n",
    "# 5. Configurer le système de récupération avec Langchain\n",
    "qa_system = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever()\n",
    ")\n",
    "\n",
    "# 6. Fonction de démonstration : Posez une question et générez une réponse\n",
    "def ask_question(query):\n",
    "    return qa_system.run(query)\n",
    "\n",
    "# Exemple de question\n",
    "response = ask_question(\"What is this document about?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
